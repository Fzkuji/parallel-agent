# Baseline Pretrained Model Evaluation Results
# Dataset: SQuAD
# Model: Qwen/Qwen2.5-7B-Instruct
# Eval Samples: 100
# Questions per Context: 5-10
# Date: 2026-01-24

Strategy        |     EM |     F1 | Lenient | Q/Ctx |  PromptTok |   GenTok | PromptTok_API | GenTok_API |   DepTok |  Latency
------------------------------------------------------------------------------------------------------------------------------
all_in_one      |  0.759 |  0.850 |   0.953 |   5.3 |      381.1 |     74.4 |         381.1 |       74.4 |      0.0 |   0.62s
sequential      |  0.813 |  0.908 |   0.951 |   5.3 |      349.1 |     65.9 |        1745.4 |       65.9 |      0.0 |   0.56s
batch           |  0.817 |  0.912 |   0.957 |   5.3 |      588.4 |     65.4 |        1308.9 |       65.4 |      0.0 |   0.16s
collab_llm      |  0.826 |  0.919 |   0.963 |   5.3 |      465.4 |     65.6 |        2311.5 |       94.0 |    277.6 |   1.03s

## Key Observations:

1. **Accuracy Ranking**: collab_llm (0.826) > batch (0.817) > sequential (0.813) > all_in_one (0.759)
   - All strategies achieve high accuracy (>75% EM)
   - Collaborative LLM strategy performs best

2. **Token Efficiency**:
   - PromptTok (deduplicated): sequential (349.1) is most efficient, collab_llm (465.4) adds dependency overhead
   - PromptTok_API (actual): batch (1308.9) < sequential (1745.4) < collab_llm (2311.5)
   - collab_llm has additional DepTok (277.6) for dependency generation

3. **Latency**:
   - batch is fastest (0.16s) due to parallel processing
   - sequential (0.56s) and all_in_one (0.62s) are moderate
   - collab_llm is slowest (1.03s) due to dependency generation overhead

4. **Quality vs Efficiency Trade-off**:
   - batch: Best balance of accuracy (0.817) and speed (0.16s)
   - collab_llm: Highest accuracy (0.826) but slowest (1.03s) and most token-intensive
   - sequential: Good accuracy (0.813) with moderate speed and token usage
